import os
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from crawl_utils import scroll_page

def crawl_quan(quan, url):
    """Crawl d·ªØ li·ªáu nh√† ƒë·∫•t c·ªßa m·ªôt qu·∫≠n v√† l∆∞u v√†o file CSV"""

    # C·∫•u h√¨nh tr√¨nh duy·ªát Chrome
    options = Options()
    options.add_argument("--headless")  # Ch·∫°y kh√¥ng m·ªü tr√¨nh duy·ªát
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920x1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # Kh·ªüi t·∫°o tr√¨nh duy·ªát
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    print(f"üîç ƒêang crawl d·ªØ li·ªáu t·ª´ {quan}...")
    driver.get(url)
    wait = WebDriverWait(driver, 10)

    data = []

    # L·∫•y t·ªïng s·ªë trang
    try:
        last_page = int(driver.find_element(By.XPATH, "//a[last()]").text.strip())
    except:
        last_page = 1  # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c th√¨ ch·ªâ crawl 1 trang

    for page in range(1, last_page + 1):
        print(f"üìå ƒêang crawl trang {page}/{last_page}...")

        # Cu·ªôn trang ƒë·ªÉ t·∫£i th√™m d·ªØ li·ªáu
        scroll_page(driver)

        # L·∫•y danh s√°ch b√†i ƒëƒÉng
        listings = wait.until(EC.presence_of_all_elements_located((By.XPATH, "//div[contains(@class, 're__card-info')]")))

        # Duy·ªát t·ª´ng b√†i ƒëƒÉng ƒë·ªÉ l·∫•y th√¥ng tin
        for listing in listings:
            try:
                title = listing.find_element(By.XPATH, ".//h3[@class='re__card-title']/span").text.strip()
                price = listing.find_element(By.XPATH, ".//span[contains(@class, 're__card-config-price')]").text.strip()
                area = listing.find_element(By.XPATH, ".//span[contains(@class, 're__card-config-area')]").text.strip()
                location = listing.find_element(By.XPATH, ".//div[@class='re__card-location']/span").text.strip()

                # Ki·ªÉm tra th√¥ng tin ph√≤ng ng·ªß
                try:
                    room = listing.find_element(By.XPATH, ".//span[contains(@class, 're__card-config-bedroom')]/span").text.strip()
                except:
                    room = "N/A"

                data.append([title, price, area, location, room])
            except Exception as e:
                print("‚ö†Ô∏è L·ªói khi l·∫•y d·ªØ li·ªáu:", e)
                continue

        # N·∫øu kh√¥ng ph·∫£i trang cu·ªëi c√πng, chuy·ªÉn sang trang ti·∫øp theo
        if page < last_page:
            try:
                next_page = driver.find_element(By.XPATH, f"//a[text()='{page + 1}']")
                driver.execute_script("arguments[0].click();", next_page)
                time.sleep(3)  # ƒê·ª£i trang t·∫£i
            except:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trang ti·∫øp theo, d·ª´ng crawl.")
                break

    # ƒê√≥ng tr√¨nh duy·ªát
    driver.quit()

    # Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p
    unique_data = list(set(tuple(row) for row in data))

    # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n file
    os.makedirs("data/raw", exist_ok=True)
    file_path = f"data/raw/{quan}.csv"

    # L∆∞u d·ªØ li·ªáu v√†o file CSV
    new_df = pd.DataFrame(unique_data, columns=["Ti√™u ƒë·ªÅ", "Gi√°", "Di·ªán t√≠ch", "V·ªã tr√≠", "Ph√≤ng"])
    new_df.to_csv(file_path, index=False, encoding="utf-8")

    print(f"‚úÖ D·ªØ li·ªáu t·ª´ {quan} ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {file_path}. T·ªïng s·ªë b√†i ƒëƒÉng: {len(new_df)}")